{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b974298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "from torchvision import datasets\n",
    "\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af65167b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, in_chans=3, embed_dims=768):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) **2\n",
    "        \n",
    "        self.proj = nn.Conv2d(\n",
    "            in_chans,\n",
    "            embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size)\n",
    "    def forward(self, x):\n",
    "        x = self.proj(\n",
    "            x\n",
    "        )\n",
    "        x = x.flatten(2)\n",
    "        x = x.transpose(1, 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "490e938a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, n_heads=12, qkv_bias=True, attn_p=0., proj_p=0.):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.dim = dim\n",
    "        self.jead_dim = dim\n",
    "        self.scale = self.head_dim ** -.5\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias = qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_p)\n",
    "        self.proj_drop = nn.Dropout(proj_p)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        n_samples, n_tokens, dim = x.shape\n",
    "        \n",
    "        if dim != self.dim:\n",
    "            raise ValueError\n",
    "        \n",
    "        qkv = self.qkv(x)\n",
    "        \n",
    "        qkv = self.qkv(x)\n",
    "        qkv = qkv.reshape(n_samples, n_tokens, 3, self.n_heads, self.head_dim)\n",
    "        qkv = k\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        \n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        k_t = k.transpose(-2, -1)\n",
    "        dp - (q @ k_t) * self.scale\n",
    "        attn = dp.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        \n",
    "        \n",
    "        weighted_avg = attn @ v\n",
    "        weighted_avg = weighted_avg,transpose(1, 2)\n",
    "        \n",
    "        weighted_avg = weighted_avg.flatten(2)\n",
    "        \n",
    "        x = self.proj(weighted_avg)\n",
    "        x = self.proj_drop(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40041537",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, out_features, p=0.):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act1 = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e14af156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x):\n",
    "    x = self.fc1(x)\n",
    "    x = self.act(x)\n",
    "    x = self.drop(x)\n",
    "    x = self.fc2(x)\n",
    "    x = self.drop(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b75decc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, n_heads, mlp_ratio=4.0, qkv_bias=True, p=0., attn_p=0.):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.ayerNorm(dim, eps=1e-6)\n",
    "        self.attn = Attnetion(\n",
    "            dim,\n",
    "            n_heads=n_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            attn_p=attn_p,\n",
    "            proj_p=p\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        hidden_features = int(dim * mlp_ratio)\n",
    "        self.mlp = MLP(\n",
    "            ub_features=dim,\n",
    "            hideden_features=hidden_features,\n",
    "            out_features=dim\n",
    "        )\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = x + self.attn(self.norm1(x))\n",
    "            x = x + self.mlpt(self.norm2(x))\n",
    "            \n",
    "            return x\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b6194a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size=384, patch_size=16, in_chans=3, \n",
    "                 n_classes=1000, embed_dim=768, depth=12, n_heads=12, \n",
    "                 mlp_ratio=4., qkv_bias=True,p=0.,attn_p=0.,\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbed(\n",
    "        img_size=img_size,\n",
    "        patch_size=patch_size,\n",
    "        in_chans=in_chans,\n",
    "        embed_dim=embed_dim,\n",
    "        )\n",
    "        self.clas_token = nn.parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(\n",
    "            torch.zeros(1, 1 + self.patch_embed.n_patches, embed_dim)\n",
    "        )\n",
    "        self.pos_drop = nn.Dropout(p=p)\n",
    "        \n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                Blocks(\n",
    "                    dim=embed_dim,\n",
    "                n_heads=n_heads,\n",
    "                mlpt_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                p=p,\n",
    "                attn_p=attn_p,\n",
    "                )\n",
    "                for _ in range(depth)    \n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.norm = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        self.head = nn.Linear(embed_dim, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        n_samples = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "        \n",
    "        cls_token = self.cls_token.expand(\n",
    "                n_samples, -1, -1\n",
    "        )\n",
    "        x = torch.cat((cls_token, x), dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        x - self.pos_drop(x)\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.norm(x)\n",
    "        cls_token_final = x[:, 0]\n",
    "        x = self.head(cls_token_final)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f44ec939",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Couldn't find any class folder in ./256x256_NormalizedWhaleImages.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-79f7e7e4e1bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;31m#Load the images\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m images = {i: datasets.ImageFolder(os.path.join(image_path, i), preprocess[i])\n\u001b[0m\u001b[0;32m     27\u001b[0m           for i in [filePath, 'test_images_100']}\n\u001b[0;32m     28\u001b[0m loader = {x: torch.utils.data.DataLoader(images[x], batch_size = 32, shuffle = True, num_workers = 0) \n",
      "\u001b[1;32m<ipython-input-3-79f7e7e4e1bf>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;31m#Load the images\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m images = {i: datasets.ImageFolder(os.path.join(image_path, i), preprocess[i])\n\u001b[0m\u001b[0;32m     27\u001b[0m           for i in [filePath, 'test_images_100']}\n\u001b[0;32m     28\u001b[0m loader = {x: torch.utils.data.DataLoader(images[x], batch_size = 32, shuffle = True, num_workers = 0) \n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[0;32m    308\u001b[0m         \u001b[0mis_valid_file\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     ):\n\u001b[1;32m--> 310\u001b[1;33m         super().__init__(\n\u001b[0m\u001b[0;32m    311\u001b[0m             \u001b[0mroot\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m             \u001b[0mloader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[0;32m    143\u001b[0m     ) -> None:\n\u001b[0;32m    144\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m         \u001b[0mclasses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    146\u001b[0m         \u001b[0msamples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[1;34m(self, directory)\u001b[0m\n\u001b[0;32m    217\u001b[0m             \u001b[1;33m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0meach\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m         \"\"\"\n\u001b[1;32m--> 219\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    220\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[1;34m(directory)\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[0mclasses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Couldn't find any class folder in {directory}.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[0mclass_to_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mcls_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Couldn't find any class folder in ./256x256_NormalizedWhaleImages."
     ]
    }
   ],
   "source": [
    "filePath = '256x256_NormalizedWhaleImages'\n",
    "\n",
    "#Preprocessing\n",
    "preprocess = {\n",
    "    filePath : transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.4282, 0.4800, 0.5347], std = [0.1421, 0.1444, 0.158])\n",
    "    ]),\n",
    "    \n",
    "    'test_images_100': transforms.Compose([\n",
    "        transforms.Resize(224), #Resize to image size\n",
    "        transforms.CenterCrop(224), #Tensor size\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.4282, 0.4800, 0.5347], std = [0.1421, 0.1444, 0.158])\n",
    "        \n",
    "    ])\n",
    "    \n",
    "}\n",
    "\n",
    "image_path = \"./\"\n",
    "\n",
    "#Load the images\n",
    "images = {i: datasets.ImageFolder(os.path.join(image_path, i), preprocess[i])\n",
    "          for i in [filePath, 'test_images_100']}\n",
    "loader = {x: torch.utils.data.DataLoader(images[x], batch_size = 32, shuffle = True, num_workers = 0) \n",
    "                for x in [filePath, 'test_images_100']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7736bbcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
