{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torchvision.models as models\n",
    "\n",
    "from torchvision import datasets\n",
    "\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = np.array(glob(\"./256x256_NormalizedWhaleImages/*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing (will add test images later) (Augmentation will be removed later once run with entire dataset)\n",
    "preprocess = {\n",
    "    '256x256_NormalizedWhaleImages': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224), #Tensor size\n",
    "        transforms.RandomHorizontalFlip(), #Augment the data to work with a small data set size (100 right now)\n",
    "        transforms.RandomRotation(15), #Augment the data\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean = [0.5, 0.5, 0.5], std = [0.5, 0.5, 0.5])\n",
    "    ]),\n",
    "    \n",
    "    'test_images_100': transforms.Compose([\n",
    "        transforms.Resize(256), #Resize to image size\n",
    "        transforms.CenterCrop(224), #Tensor size\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std = [0.5, 0.5, 0.5])\n",
    "        \n",
    "    ])\n",
    "    \n",
    "}\n",
    "\n",
    "image_path = \"./\"\n",
    "\n",
    "#Load the images\n",
    "images = {i: datasets.ImageFolder(os.path.join(image_path, i), preprocess[i])\n",
    "          for i in ['256x256_NormalizedWhaleImages']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(images[x], batch_size = 20, shuffle = True, num_workers = 0) \n",
    "                for x in ['256x256_NormalizedWhaleImages']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making the CNN\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        #Layer 1\n",
    "        self.layer1 =  nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, padding = 1),\n",
    "            nn.BatchNorm2d(224, 3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        #Layer 2\n",
    "        self.layer2 =  nn.Sequential(\n",
    "            nn.Conv2d(16, 32, 3, padding = 1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        #Layer 3\n",
    "        self.layer3 =  nn.Sequential(\n",
    "            nn.Conv2d(32, 64, 3, padding = 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        #Layer 4\n",
    "        self.layer4 =  nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 3, padding = 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        #Layer 5\n",
    "        self.layer5 =  nn.Sequential(\n",
    "            nn.Conv2d(128, 256, 3, padding = 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(2)\n",
    "            \n",
    "        )\n",
    "        \n",
    "        self.final_bn = nn.BatchNorm2d(256)\n",
    "        \n",
    "        #FC Layer\n",
    "        self.drop = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(256*7*7, 512)\n",
    "        self.fc2 = nn.Linear(512, len(images['256x256_NormalizedWhaleImages'].classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Forward function\n",
    "def forward(self, x):\n",
    "    logits = self.layer1(x)\n",
    "    logits = self.layer2(logits)\n",
    "    logits = self.layer3(logits)\n",
    "    logits = self.layer4(logits)\n",
    "    logits = self.layer5(logits)\n",
    "    \n",
    "    logits = self.final_bn(logits)\n",
    "    \n",
    "    logits = self.drop(logits)\n",
    "    \n",
    "    logits = self.fc1(logits)\n",
    "    logits = self.drop(logits)\n",
    "    logits = self.fc2(logits)\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiating a CNN\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d61055791f549c65e2e2db88b496de78efc957c9b2b96307a2737816c071efe3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
